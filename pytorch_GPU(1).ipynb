{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lima-breno/deep_learning_frameworks/blob/main/pytorch_GPU(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#0- Verificar se vc tem a GPU disponível"
      ],
      "metadata": {
        "id": "Ad5UECqbJ33l"
      },
      "id": "Ad5UECqbJ33l"
    },
    {
      "cell_type": "markdown",
      "id": "501448dd",
      "metadata": {
        "id": "501448dd"
      },
      "source": [
        "Verifique a instalação da GPU com o comando abaixo. Se a GPU estiver disponível, você verá informações sobre o dispositivo CUDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b5330a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e6b5330a",
        "outputId": "392e559f-7fb2-4aa2-97de-0417e75ea6d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "torch.__version__\n",
        "\n",
        "#Primeiro vejo se tenho uma versão que suporta o cuda."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#depois verifico que se nho GPU disponível no ambiente\n",
        "torch.cuda.is_available()\n",
        "# o resultado indica que não tenho! Api nesse caso preciso habilitar a GPU.\n",
        "## Vou em Editar/Configurações de notebook/ seleciono a GPU T4 (que o plano free tem acesso)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3D84ebyH_Xa",
        "outputId": "63683c30-5a76-42c9-8449-3c7d7cc7d61d"
      },
      "id": "o3D84ebyH_Xa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#agora importo novamente a biblitoteca e vejo se a GPU está valendo\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwu57QVzLd6P",
        "outputId": "36714546-748a-4756-e221-c10cbc492ad1"
      },
      "id": "vwu57QVzLd6P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28388ea0",
      "metadata": {
        "id": "28388ea0"
      },
      "source": [
        "# 1 - Instalar o PyTorch e o CUDA: (CASO EU N TENHA A VERSAO DO CUDA!)\n",
        "Certifique-se de ter o PyTorch instalado e configurado corretamente com suporte ao CUDA, que é a plataforma de computação paralela da NVIDIA usada para executar cálculos em GPUs NVIDIA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0314c8c8",
      "metadata": {
        "id": "0314c8c8"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f96a986",
      "metadata": {
        "id": "2f96a986"
      },
      "source": [
        "# 2 - Inicializar Tensores na GPU:\n",
        "Para tirar proveito da GPU, você precisa criar tensores diretamente na GPU. Você pode fazer isso especificando o dispositivo ao criar o tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f179af70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f179af70",
        "outputId": "92375257-677b-4265-ca40-29c6c017b4df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#Criando o dispositivo (hardware que estarei rodando) por meio de duas formas\n",
        "#Forma 01: pra ter acesso a CPU que estamos lidando\n",
        "torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Forma 02: utilizando a GPU (chamando o cuda -> é a maneira certa)\n",
        "torch.device('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4vDBja0Mwr1",
        "outputId": "217217c9-1a64-47bc-f061-d9613a4e4103"
      },
      "id": "l4vDBja0Mwr1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Forma 03: Posso fazer uma criaçao CONDICIONAL, eu valido se tenho uma GPU, se tiver eu instacio como GPU, caso n, instancio como CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#é o melhor!"
      ],
      "metadata": {
        "id": "suTPj0sbM8ad"
      },
      "id": "suTPj0sbM8ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Posso criar os tensores da seguinte maneira"
      ],
      "metadata": {
        "id": "jSj3p5YpQFsf"
      },
      "id": "jSj3p5YpQFsf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c92e11d3",
      "metadata": {
        "id": "c92e11d3"
      },
      "outputs": [],
      "source": [
        "#FORMA 01:\n",
        "##Criando um tensor aleatório para demonstrar se está sendo utilizada a GPU\n",
        "\n",
        "tensor_1 = torch.Tensor([1,2,3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1.device\n",
        "#Ele está na CPU, preciso mover para a GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flASunhOOGJZ",
        "outputId": "f193692b-ba2f-4eae-e798-40bd63469cdf"
      },
      "id": "flASunhOOGJZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Posso chamar um método para passar para a GPU\n",
        "tensor_1.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLEywDTOOP0H",
        "outputId": "bd79964e-2263-4e01-fcc9-544a750ab576"
      },
      "id": "SLEywDTOOP0H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Aqui ele cria uma cópia do dado que está dentro do tensor, joga essa info lá na GPU e essa cópia tem uma referencia ao dado no tensor da cpu (ideia de ponteiros).\n",
        "##Só que para eu poder usar este cara eu preciso atribuir ele a alguem.\n"
      ],
      "metadata": {
        "id": "HOddLyNgOPxV"
      },
      "id": "HOddLyNgOPxV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FORMA 02:\n",
        "##\n",
        "tensor_2 = torch.tensor([1,2,3], device = device, dtype= torch.float64)\n",
        "tensor_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy5PcTS6PWRX",
        "outputId": "a475f483-1fd3-4b7f-b582-ec6d86026c53"
      },
      "id": "Dy5PcTS6PWRX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc349521",
      "metadata": {
        "id": "bc349521"
      },
      "source": [
        "Ou você pode mover um tensor existente para a GPU usando o método `to()`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_on_gpu = tensor_1.to(device)\n",
        "tensor_on_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OlYLLB1QK5u",
        "outputId": "fc96e465-424e-488f-c461-aecdeea65c8f"
      },
      "id": "7OlYLLB1QK5u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#qualquer operaao com o tensor\n",
        "\n",
        "tensor_1 = tensor_1*10\n",
        "tensor_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQUOHCZ0Qk_R",
        "outputId": "dcbd4fff-c7eb-4e2c-b811-1f9c730aaf0e"
      },
      "id": "KQUOHCZ0Qk_R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10., 20., 30.])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#se eu tentar multiplicar escalarmente tensor na gpu com um tensor de 64 bits, dá erro! (tensores com precisoes riferentes float x double)\n",
        "\n",
        "tensor_on_gpu @ tensor_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "t9mZorl1RZTq",
        "outputId": "05114bf5-f67c-49cd-d0f8-884bb7f04f9e"
      },
      "id": "t9mZorl1RZTq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "dot : expected both vectors to have same dtype, but found Float and Double",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-30-1443736052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#se eu tentar multiplicar escalarmente tensor na gpu com um tensor de 64 bits, dá erro! (tensores com precisoes riferentes float x double)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtensor_on_gpu\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtensor_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: dot : expected both vectors to have same dtype, but found Float and Double"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dá erro pq:\n",
        "tensor_on_gpu.dtype, tensor_2.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbT2LMhtSCBd",
        "outputId": "773c65b3-6691-4d42-96b6-53fa2d9fe148"
      },
      "id": "KbT2LMhtSCBd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float32, torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#para resolver este problema eu preciso converter tudo pra float ou tudo para double (64 bits)\n",
        "\n",
        "#PRIMEIRO TIPO DE CONVERSAO\n",
        "tensor_on_gpu.double() @ tensor_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-HdPWDjRnJy",
        "outputId": "743fc649-0338-4857-8fe0-0c052d72cd62"
      },
      "id": "T-HdPWDjRnJy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(14., device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SEGUNDO TIPO DE CONVERSAO\n",
        "tensor_on_gpu @ tensor_2.float()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7Kpk4Z8SkxA",
        "outputId": "d828dc88-c8d9-4086-bfa5-e336a4b6cb6b"
      },
      "id": "s7Kpk4Z8SkxA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(14., device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# O uso da GPU quando comparado com a CPU gera um ganho de processamento absurdo!"
      ],
      "metadata": {
        "id": "C7Am820sUH7T"
      },
      "id": "C7Am820sUH7T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5d6a8334",
      "metadata": {
        "id": "5d6a8334"
      },
      "source": [
        "# 3 - Mover Modelos para a GPU:\n",
        "Se você estiver treinando um modelo, você também deve mover o modelo para a GPU. Faça isso chamando o método to() no modelo. Primeiramente, os tensores (dataset) devem estar na GPU,o que já fora feito anteriormente.\n",
        "\n",
        "\n",
        "OBS: A classe n precisa ser utilizada na GPU, somente o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a571b6fa",
      "metadata": {
        "id": "a571b6fa"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeuModelo, self).__init__()\n",
        "        self.camada = nn.Linear(10, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.camada(x)\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93377476",
      "metadata": {
        "id": "93377476"
      },
      "source": [
        "Isso move todos os parâmetros do modelo para a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "717e3722",
      "metadata": {
        "id": "717e3722"
      },
      "source": [
        "# 4 - Treinar na GPU:\n",
        "Quando você executa operações em tensores na GPU, o PyTorch automaticamente executa essas operações na GPU. Certifique-se de que os dados de entrada e os rótulos também estejam na GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9cc4d2d",
      "metadata": {
        "id": "c9cc4d2d"
      },
      "outputs": [],
      "source": [
        "dados_de_treino = dados_de_treino.to(device)\n",
        "rotulos_de_treino = rotulos_de_treino.to(device)\n",
        "\n",
        "# Treinar seu modelo\n",
        "saida = modelo(dados_de_treino)\n",
        "perda = criterio(saida, rotulos_de_treino)\n",
        "perda.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c064f577",
      "metadata": {
        "id": "c064f577"
      },
      "source": [
        "## Treinando um simples modelo na GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76cf67e4",
      "metadata": {
        "id": "76cf67e4"
      },
      "source": [
        "Vamos criar um exemplo simples com dados aleatórios, um modelo básico e treinamento na GPU usando PyTorch. Neste exemplo, usaremos um modelo de regressão linear simples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25e65f26",
      "metadata": {
        "id": "25e65f26"
      },
      "source": [
        "### Definições iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfac0d45",
      "metadata": {
        "id": "bfac0d45"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Gerar dados aleatórios\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Dados\n",
        "num_samples = 10000\n",
        "input_size = 5\n",
        "output_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3afcef62",
      "metadata": {
        "id": "3afcef62"
      },
      "source": [
        "### Criação dos dados de treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6aa951",
      "metadata": {
        "id": "fe6aa951"
      },
      "outputs": [],
      "source": [
        "X_train = torch.rand((num_samples, input_size)).float()\n",
        "y_train = 3 * X_train[:, 0] - 2 * X_train[:, 1] + 1.5 * X_train[:, 2] + torch.randn(num_samples) #gerando uma regressao linear baseada no dado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#como saber a qtd de memoria que estou usando\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bptsurDVnT2",
        "outputId": "4a4c51cc-517b-4926-b906-74c19ba7b596"
      },
      "id": "0bptsurDVnT2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 20 15:00:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0             32W /   70W |     150MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ac841c",
      "metadata": {
        "id": "e1ac841c"
      },
      "source": [
        "### Definição do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157eb74c",
      "metadata": {
        "id": "157eb74c"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.camada = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.camada(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = LinearRegression(input_size, output_size)#posso criar o modelo e colocar o .to(device) para evitar problemas na frente\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterio = nn.MSELoss()\n",
        "otimizador = optim.SGD(modelo.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "QF-0LU6iV5yX"
      },
      "id": "QF-0LU6iV5yX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9d2ade1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9d2ade1",
        "outputId": "a4216606-f978-4605-b6d7-ef8601061bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/1000], Perda: 2.2640\n",
            "Época [200/1000], Perda: 2.2630\n",
            "Época [300/1000], Perda: 2.2621\n",
            "Época [400/1000], Perda: 2.2613\n",
            "Época [500/1000], Perda: 2.2607\n",
            "Época [600/1000], Perda: 2.2601\n",
            "Época [700/1000], Perda: 2.2596\n",
            "Época [800/1000], Perda: 2.2592\n",
            "Época [900/1000], Perda: 2.2589\n",
            "Época [1000/1000], Perda: 2.2586\n",
            "CPU times: user 7.46 s, sys: 482 ms, total: 7.94 s\n",
            "Wall time: 8.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#enviar o modelo para GPU\n",
        "modelo = modelo.to(device)\n",
        "\n",
        "#enviar dados para GPU\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    saida = modelo(X_train)\n",
        "    perda = criterio(saida, y_train)\n",
        "\n",
        "    # Backward pass e otimização\n",
        "    otimizador.zero_grad()\n",
        "    perda.backward()\n",
        "    otimizador.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Época [{epoch + 1}/{num_epochs}], Perda: {perda.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testando na CPU\n",
        "modelo = LinearRegression(input_size, output_size)\n",
        "device = torch.device(\"cpu\")\n",
        "criterio = nn.MSELoss()\n",
        "otimizador = optim.SGD(modelo.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "P7uRA0H9X6b5"
      },
      "id": "P7uRA0H9X6b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Considerando o modelo na CPU\n",
        "%%time\n",
        "\n",
        "#enviar dados para GPU\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    saida = modelo(X_train)\n",
        "    perda = criterio(saida, y_train)\n",
        "\n",
        "    # Backward pass e otimização\n",
        "    otimizador.zero_grad()\n",
        "    perda.backward()\n",
        "    otimizador.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Época [{epoch + 1}/{num_epochs}], Perda: {perda.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ngo4we6XU_b",
        "outputId": "ffba6b8d-c001-4df9-fd59-715995f7261b"
      },
      "id": "8Ngo4we6XU_b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/1000], Perda: 2.2807\n",
            "Época [200/1000], Perda: 2.2751\n",
            "Época [300/1000], Perda: 2.2711\n",
            "Época [400/1000], Perda: 2.2680\n",
            "Época [500/1000], Perda: 2.2657\n",
            "Época [600/1000], Perda: 2.2639\n",
            "Época [700/1000], Perda: 2.2625\n",
            "Época [800/1000], Perda: 2.2615\n",
            "Época [900/1000], Perda: 2.2606\n",
            "Época [1000/1000], Perda: 2.2599\n",
            "CPU times: user 4min 33s, sys: 6min 12s, total: 10min 46s\n",
            "Wall time: 10min 50s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Obs: o código acima n envia para a GPU!!!!\n",
        "## DEMOROU MUITO MAIS PARA RODAR NA CPU!!\n",
        "# o modelo TPU tem muito mais RAM, preciso adaptar daí!"
      ],
      "metadata": {
        "id": "vHQEImo5ZnD_"
      },
      "id": "vHQEImo5ZnD_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação do modelo GPU\n",
        "modelo.eval()\n",
        "X_test = torch.rand((10, input_size)).float().to(device)\n",
        "previsoes = modelo(X_test)\n",
        "print(\"\\nPrevisões do Modelo:\")\n",
        "print(previsoes.detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "dFiGWVq4W5Up"
      },
      "id": "dFiGWVq4W5Up",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "65956e67",
      "metadata": {
        "id": "65956e67"
      },
      "source": [
        "Este exemplo cria dados de treinamento com uma relação linear específica, define um modelo de regressão linear, move o modelo e os dados para a GPU (se disponível), e realiza o treinamento usando a GPU. Note que este é um exemplo muito simples e pode não refletir um cenário real, mas é útil para entender o processo básico de treinamento na GPU com PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc21dd29",
      "metadata": {
        "id": "cc21dd29"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}